{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from data.garment_dataset import GarmentDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import json\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "from accelerate import notebook_launcher\n",
    "from diffusers import AutoencoderKL\n",
    "from pprint import pprint\n",
    "from utils.uv_tools import apply_displacement\n",
    "from utils.visualization import imshow\n",
    "import wandb, random\n",
    "from losses.losses import uv_loss\n",
    "from utils.geometry import l2dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "torch.random.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/home/adumouli/Bureau/garment-diffusion/configs/config.json\"\n",
    "with open(config_file) as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RESET\n",
    "# vae_orig = AutoencoderKL.from_pretrained(config[\"vae\"][\"folder\"]) # we use this one\n",
    "# # vae_orig = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\") # we use this one\n",
    "# # vae_orig = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\")\n",
    "# # vae_orig = AutoencoderKL.from_pretrained(\"ostris/OpenFLUX.1\", subfolder='vae')\n",
    "# # vae_orig = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder='vae')\n",
    "\n",
    "# # vae_orig = AutoencoderKL(\n",
    "# #     in_channels=3,\n",
    "# #     out_channels=3,\n",
    "# #     down_block_types=[\n",
    "# #         \"DownEncoderBlock2D\",\n",
    "# #         \"DownEncoderBlock2D\",\n",
    "# #         \"DownEncoderBlock2D\",\n",
    "# #         \"DownEncoderBlock2D\"\n",
    "# #     ],\n",
    "# #     up_block_types=[\n",
    "# #         \"UpDecoderBlock2D\",\n",
    "# #         \"UpDecoderBlock2D\",\n",
    "# #         \"UpDecoderBlock2D\",\n",
    "# #         \"UpDecoderBlock2D\"\n",
    "# #     ],\n",
    "# #     block_out_channels=[64, 128, 256, 256],\n",
    "# #     layers_per_block=2,\n",
    "# #     act_fn='silu',\n",
    "# #     latent_channels=16,\n",
    "# #     norm_num_groups=32,\n",
    "# #     sample_size=config[\"model\"][\"image_size\"],\n",
    "# #     scaling_factor=0.3611,\n",
    "# #     shift_factor=0,\n",
    "# #     force_upcast=False,\n",
    "# #     use_quant_conv=False,\n",
    "# #     use_post_quant_conv=False,\n",
    "# #     mid_block_add_attention=True\n",
    "# # )\n",
    "# print(\"VAE network size: \" + str(vae_orig.num_parameters()/1e6))\n",
    "# pprint(vae_orig.config)\n",
    "\n",
    "# # vae_orig.save_pretrained(config[\"vae\"][\"folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval(vae, dataset):\n",
    "    mask = ~GarmentDataset(config, device='cuda').mask\n",
    "    vae_orig = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", subfolder='vae').to(vae.device)\n",
    "\n",
    "    img = dataset[500]['vdm'].to(vae.device)\n",
    "    encode = vae.encode(img.permute(2,0,1)[None,...]).latent_dist.mode()\n",
    "    decode = vae.decode(encode).sample[0].permute(1,2,0)\n",
    "\n",
    "    dist = torch.abs(decode-img)\n",
    "    dist[mask]*=0\n",
    "    print(dist.mean())\n",
    "    print(dist.max())\n",
    "\n",
    "    encode = vae_orig.encode(img.permute(2,0,1)[None,...]).latent_dist.mode()\n",
    "    decode2 = vae_orig.decode(encode).sample[0].permute(1,2,0)\n",
    "    \n",
    "    dist2 = torch.abs(decode2-img)\n",
    "    dist2[mask]*=0\n",
    "    print(dist2.mean())\n",
    "    print(dist2.max())\n",
    "\n",
    "    dist3 = torch.abs(decode-decode2)\n",
    "    print(dist3.mean())\n",
    "    print(dist3.max())\n",
    "\n",
    "    # imshow(torch.cat((decode, img, decode2), 1))\n",
    "    # imshow(torch.cat((dist, dist2, dist3), 1)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_finetune(config):\n",
    "\n",
    "    eval_generator = torch.Generator(device='cpu').manual_seed(0)\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(config[\"vae\"][\"folder\"]).to('cuda')\n",
    "\n",
    "    loss = dict()\n",
    "    dataset = GarmentDataset(config, device='cuda')\n",
    "    # dataset = VAETrainingDataset(dataset)\n",
    "    train_dataset, eval_dataset = random_split(dataset, [0.95, .05], generator=eval_generator)\n",
    "\n",
    "    mask = ~dataset.mask.expand(config[\"vae\"][\"batch_size\"], -1, -1)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"vae\"][\"batch_size\"], drop_last=True, num_workers=4, shuffle=True, persistent_workers=True, pin_memory=False)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(vae.parameters(), lr=config[\"vae\"][\"lr\"] * config[\"vae\"][\"batch_size\"])\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config['mixed_precision'],\n",
    "        gradient_accumulation_steps=1,\n",
    "        log_with=\"wandb\",\n",
    "        project_dir=os.path.join(config[\"vae\"][\"folder\"], \"logs\")\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(config[\"vae\"][\"folder\"], exist_ok=True)\n",
    "        accelerator.init_trackers(\"vae\",\n",
    "            config={\"config\": config[\"vae\"]}\n",
    "            )\n",
    "    \n",
    "    optimizer, train_dataloader, vae, vae.encoder, vae.decoder = accelerator.prepare(\n",
    "        optimizer, train_dataloader, vae, vae.encoder, vae.decoder\n",
    "    )\n",
    "\n",
    "    vae.requires_grad_(True)\n",
    "    vae.train()\n",
    "\n",
    "    global_step = 0\n",
    "    progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    # Now you train the model\n",
    "    for epoch in range(config[\"vae\"][\"max_epochs\"]):\n",
    "        progress_bar.reset()\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            with accelerator.accumulate(vae):\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                clean_images = batch['vdm'].to(accelerator.device)\n",
    "                clean_vertices = batch['cloth_vertices'].to(accelerator.device)\n",
    "\n",
    "                encode = vae.encode(clean_images.permute(0,3,1,2)).latent_dist\n",
    "                decode = vae.decode(encode.sample()).sample.permute(0,2,3,1)\n",
    "\n",
    "                # reconstruction loss\n",
    "                rec_loss = F.mse_loss(decode.float(), clean_images.float(), reduction='none')\n",
    "                rec_loss[mask] *= 0.1\n",
    "                loss['rec_loss'] = rec_loss.mean()\n",
    "\n",
    "                decoded_vertices = apply_displacement(dataset.template, decode)\n",
    "                mesh_loss = F.mse_loss(decoded_vertices.float(), clean_vertices.float(), reduction='none')\n",
    "                loss['mesh_loss'] = mesh_loss.mean()\n",
    "                \n",
    "                loss['uv_loss'] = uv_loss(decode.float(), dataset, decoded_vertices)\n",
    "                loss['uv_loss'][mask] *= 0.1\n",
    "                loss['uv_loss'] = loss['uv_loss'].mean()\n",
    "\n",
    "                # Make the KL divergence loss (effectively regularize the latents)\n",
    "                loss['kl_loss'] = encode.kl().mean()\n",
    "\n",
    "                # Combine the losses\n",
    "                loss['total_loss'] = loss['rec_loss'] + loss['mesh_loss'] + config[\"vae\"][\"kl_loss\"] * loss['kl_loss'] + loss['uv_loss'] * config[\"vae\"][\"uv_loss\"]\n",
    "\n",
    "                accelerator.backward(loss['total_loss'])\n",
    "\n",
    "                accelerator.clip_grad_norm_(vae.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            for k in loss.keys():\n",
    "                elem = loss[k]\n",
    "                if isinstance(elem, torch.Tensor):\n",
    "                    loss[k] = elem.detach().item()\n",
    "\n",
    "            logs = loss.copy()\n",
    "\n",
    "            logs['epoch'] = epoch\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            progress_bar.set_postfix(**loss)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # After each epoch you optionally evaluate and save the model\n",
    "            if accelerator.is_main_process and step%config[\"vae\"][\"eval_freq\"] == 0:\n",
    "                \n",
    "                eval_vae = accelerator.unwrap_model(vae)\n",
    "                eval_vae.save_pretrained(config[\"vae\"][\"folder\"])\n",
    "\n",
    "                # eval\n",
    "                with torch.inference_mode():\n",
    "\n",
    "                    indices = torch.randint(high=len(eval_dataset), size=(config[\"validation\"][\"batch_size\"],), generator=eval_generator)\n",
    "                    ground_truth = torch.stack([eval_dataset[i]['vdm'] for i in indices])\n",
    "                    ground_truth = ground_truth.to(accelerator.device).permute(0,3,1,2)\n",
    "                    gt_mesh = torch.stack([eval_dataset[i]['cloth_vertices'] for i in indices])\n",
    "                    gt_mesh = gt_mesh.to(accelerator.device)\n",
    "\n",
    "                    encode = eval_vae.encode(ground_truth).latent_dist.mode()\n",
    "                    decode = eval_vae.decode(encode).sample\n",
    "\n",
    "                    dist = torch.abs(decode-ground_truth)\n",
    "\n",
    "                    decoded_vertices = apply_displacement(dataset.template, decode.permute(0,2,3,1))\n",
    "                    mesh_loss = F.mse_loss(decoded_vertices.float(), gt_mesh.float(), reduction='none')\n",
    "\n",
    "                    eval = dict()\n",
    "                    eval['eval_mesh_mean'] = mesh_loss.mean().detach().item()\n",
    "                    eval['eval_mesh_max'] = mesh_loss.max().detach().item()\n",
    "                    eval['eval_dist_mean'] = dist.mean().detach().item()\n",
    "                    eval['eval_dist_max'] = dist.max().detach().item()\n",
    "\n",
    "                    accelerator.log(eval, step=global_step)\n",
    "\n",
    "                del eval_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = (config, )\n",
    "# notebook_launcher(vae_finetune, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_finetune(config):\n",
    "\n",
    "    eval_generator = torch.Generator(device='cpu').manual_seed(0)\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(config[\"vae\"][\"folder\"])\n",
    "\n",
    "    loss = dict()\n",
    "    dataset = GarmentDataset(config, device='cpu')\n",
    "    train_dataset, eval_dataset = random_split(dataset, [0.95, .05], generator=eval_generator)\n",
    "\n",
    "    mask = ~dataset.mask.expand(config[\"vae\"][\"batch_size\"], -1, -1)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config[\"vae\"][\"batch_size\"], drop_last=True, num_workers=4, shuffle=True, persistent_workers=True, pin_memory=True)\n",
    "   \n",
    "    optimizer = torch.optim.AdamW(vae.decoder.parameters(), lr=config[\"vae\"][\"lr\"] * config[\"vae\"][\"batch_size\"])\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config['mixed_precision'],\n",
    "        gradient_accumulation_steps=1,\n",
    "        log_with=\"wandb\",\n",
    "        project_dir=os.path.join(config[\"vae\"][\"folder\"], \"logs\")\n",
    "    )\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        os.makedirs(config[\"vae\"][\"folder\"], exist_ok=True)\n",
    "        accelerator.init_trackers(\"vae\",\n",
    "            config={\"config\": config[\"vae\"]}\n",
    "            )\n",
    "    \n",
    "\n",
    "    vae.decoder.requires_grad_(True)\n",
    "    vae.decoder.train()\n",
    "    vae.encoder.requires_grad_(False)\n",
    "    vae.encoder.eval()\n",
    "    \n",
    "    vae.to(accelerator.device)\n",
    "\n",
    "    vae.encode = torch.compile(vae.encode, mode=\"max-autotune\", fullgraph=True, dynamic=True)\n",
    "\n",
    "    optimizer, train_dataloader, vae.decoder = accelerator.prepare(\n",
    "        optimizer, train_dataloader, vae.decoder\n",
    "    )\n",
    "\n",
    "    global_step = 0\n",
    "    progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    # Now you train the model\n",
    "    for epoch in range(config[\"vae\"][\"max_epochs\"]):\n",
    "        progress_bar.reset()\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            clean_images = batch['vdm'].to(accelerator.device)\n",
    "            clean_vertices = batch['cloth_vertices'].to(accelerator.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                encode = vae.encode(clean_images.permute(0,3,1,2)).latent_dist\n",
    "\n",
    "            with accelerator.accumulate(vae.decoder):\n",
    "\n",
    "                decode = vae.decode(encode.sample()).sample.permute(0,2,3,1)\n",
    "\n",
    "                # reconstruction loss\n",
    "                rec_loss = F.mse_loss(decode.float(), clean_images.float(), reduction='none')\n",
    "                rec_loss[mask] *= 0.1\n",
    "                loss['rec_loss'] = rec_loss.sum()\n",
    "\n",
    "                decoded_vertices = apply_displacement(dataset.template, decode)\n",
    "                mesh_loss = l2dist(decoded_vertices.float(), clean_vertices.float())\n",
    "                loss['mesh_loss'] = mesh_loss.sum()\n",
    "\n",
    "                # loss['uv_loss'] = uv_loss(decode.float(), dataset, decoded_vertices)\n",
    "                # loss['uv_loss'][mask] *= 0.1\n",
    "                # loss['uv_loss'] = loss['uv_loss'].sum()\n",
    "\n",
    "                # Combine the losses\n",
    "                loss['total_loss'] = loss['rec_loss'] + loss['mesh_loss'] * config[\"vae\"][\"mesh_loss\"]# + loss['uv_loss'] * config[\"vae\"][\"uv_loss\"]\n",
    "\n",
    "                accelerator.backward(loss['total_loss'])\n",
    "\n",
    "                accelerator.clip_grad_norm_(vae.decoder.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            for k in loss.keys():\n",
    "                elem = loss[k]\n",
    "                if isinstance(elem, torch.Tensor):\n",
    "                    loss[k] = elem.detach().item()\n",
    "\n",
    "            logs = loss.copy()\n",
    "\n",
    "            logs['epoch'] = epoch\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            progress_bar.set_postfix(**loss)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "            if accelerator.is_main_process and step%config[\"vae\"][\"eval_freq\"] == 0:\n",
    "                \n",
    "                eval_vae = accelerator.unwrap_model(vae)\n",
    "                eval_vae.save_pretrained(config[\"vae\"][\"folder\"]+'2')\n",
    "\n",
    "                # eval\n",
    "                with torch.inference_mode():\n",
    "\n",
    "                    indices = torch.randint(high=len(eval_dataset), size=(config[\"validation\"][\"batch_size\"],), generator=eval_generator)\n",
    "                    ground_truth = torch.stack([eval_dataset[i]['vdm'] for i in indices])\n",
    "                    ground_truth = ground_truth.to(accelerator.device).permute(0,3,1,2)\n",
    "                    gt_mesh = torch.stack([eval_dataset[i]['cloth_vertices'] for i in indices])\n",
    "                    gt_mesh = gt_mesh.to(accelerator.device)\n",
    "\n",
    "                    encode = vae.encode(ground_truth).latent_dist.mode()\n",
    "                    decode = vae.decode(encode).sample\n",
    "\n",
    "                    dist = torch.abs(decode-ground_truth)\n",
    "\n",
    "                    decoded_vertices = apply_displacement(dataset.template, decode.permute(0,2,3,1))\n",
    "                    mesh_loss = F.mse_loss(decoded_vertices.float(), gt_mesh.float(), reduction='none')\n",
    "\n",
    "                    eval = dict()\n",
    "                    eval['eval_mesh_mean'] = mesh_loss.mean().detach().item()\n",
    "                    eval['eval_mesh_max'] = mesh_loss.max().detach().item()\n",
    "                    eval['eval_dist_mean'] = dist.mean().detach().item()\n",
    "                    eval['eval_dist_max'] = dist.max().detach().item()\n",
    "\n",
    "                    accelerator.log(eval, step=global_step)\n",
    "\n",
    "            if accelerator.is_main_process and step%config[\"vae\"][\"save_image_freq\"] == 0:\n",
    "\n",
    "                # eval\n",
    "                with torch.inference_mode():\n",
    "                    \n",
    "                    indice = random.randint(0, len(eval_dataset))\n",
    "                    ground_truth = eval_dataset[indice]['vdm'][None, ...]\n",
    "                    ground_truth = ground_truth.to(accelerator.device).permute(0,3,1,2)\n",
    "\n",
    "                    encode = vae.encode(ground_truth).latent_dist.mode()\n",
    "                    decode = vae.decode(encode).sample\n",
    "                    \n",
    "                    examples = []\n",
    "                    image = wandb.Image(ground_truth[0], caption=f\"Ground truth\")\n",
    "                    examples.append(image)\n",
    "                    image = wandb.Image(decode[0], caption=\"Decoded\")\n",
    "                    examples.append(image)\n",
    "\n",
    "                    accelerator.log({\"Images\": examples}, step=global_step)\n",
    "                    print(indice)\n",
    "\n",
    "                    decode = decode[0].permute(1,2,0)\n",
    "                    ground_truth = ground_truth[0].permute(1,2,0)\n",
    "                    dist = (decode-ground_truth)\n",
    "\n",
    "                    imshow(torch.cat((decode, ground_truth), 1))\n",
    "                    imshow(dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (config, )\n",
    "notebook_launcher(decoder_finetune, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(vae = AutoencoderKL.from_pretrained(config[\"vae\"][\"folder\"]).to('cuda'), dataset = GarmentDataset(config, device='cuda'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgarment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
