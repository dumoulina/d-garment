# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from typing import List, Optional, Union

import torch

from model.VDM_latent import VDMStableDiffusionPipeline, retrieve_timesteps
from data.normalization import get_normalized_body, flatten_embeddings
from data.structures import GarmentDict, Mesh
from body_models.smpl_model import BodyModel
from losses.losses import collision_loss
from utils.geometry import batch_compute_points_normals
from utils.uv_tools import apply_displacement
from utils.helpers import batchify_dict

class VDMStableDiffusionPipelineOptimize(VDMStableDiffusionPipeline):

    def __call__(
        self,
        condition: GarmentDict,
        template: Mesh = None,
        body_model: BodyModel = None,
        num_inference_steps: int = 50,
        num_opti_steps=0,
        lr: float = 0,
        clothing_weight: float = 0,
        acceleration_weight: float = 0,
        target: torch.Tensor = None,
        timesteps: List[int] = None,
        sigmas: List[float] = None,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.Tensor] = None,
        decoded: Optional[bool] = True,
        losses=dict(),
    ):
        r"""
        The call function to the pipeline for generation.
        Args:
            condition (`GarmentDict`, *optional*):
                The informations to guide image generation.
            template (`Mesh`, *optional*):
                The template mesh, needed for guidance.
            body_model (`BodyModel`, *optional*):
                The SMPL body model, needed for guidance.
            num_opti_steps (`int`, *optional*, defaults to 0):
                The number of optimization step to do.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            lr (`float`, *optional*):
                Learning rate to optimize latent.
            acceleration_weight (`float`, *optional*):
                Guidance weight to regularize acceleration (actually velocity).
            clothing_weight (`float`, *optional*):
                Guidance weight to regularize cloth-body penetration.
            target (`torch.Tensor`, *optional*):
                Last vertices positions frame to compute the acceleration (actually velocity).
            timesteps (`List[int]`, *optional*):
                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument
                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is
                passed will be used. Must be in descending order.
            sigmas (`List[float]`, *optional*):
                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in
                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed
                will be used.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (Î·) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies
                to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make
                generation deterministic.
            latents (`torch.Tensor`, *optional*):
                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts or optimization.
                If not provided, a latents tensor is generated by sampling using the supplied random `generator`.
            decoded (`bool`, *optional*, defaults to `"True"`):
                Choose between `latent` or decoded output.
            losses (`list`, *optional*):
                Will be populated with loss over time for analysis purposes.
        Returns:
            torch.Tensor: 
                Denoised and decoded image prediction if `decoded` is True else denoised latents
        """

        # 1. Prepare data
        # if len(condition["trans"].shape) == 2:
        #     for k in condition.keys():
        #         if isinstance(condition[k], torch.Tensor):
        #             condition[k] = condition[k][None, ...]
        if condition is not None:
            condition = condition.copy()
            if len(condition["trans"].shape) == 2:
                condition = batchify_dict(condition)

            bodict = {}
            bodict["betas"] = condition["betas"]
            bodict["trans"], bodict["poses"] = get_normalized_body(condition)
            bodict["trans"] = bodict["trans"][:, -1]
            bodict["poses"] = bodict["poses"][:, -1]
            body_vertices = body_model(bodict)
            body_faces = body_model.get_faces()
            del bodict
            body_normal = batch_compute_points_normals(body_vertices, body_faces)

            condition_embedding = flatten_embeddings(condition).to(self.unet.dtype)
            batch_size = condition_embedding.shape[0]
        else:
            condition_embedding = None
            batch_size = 1

        for k in ['clothing', 'std', 'mean']:
            losses[k] = []
        if target is not None:
            losses['accel'] = []

        # 3. Define call parameters
        device = self._execution_device

        # 4. Prepare timesteps
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler, num_inference_steps, device, timesteps, sigmas
        )

        # 5. Prepare latent variables
        learned_latents = self.prepare_latents(
            batch_size,
            generator,
            latents,
        )
        learned_latents = torch.nn.Parameter(learned_latents)

        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline
        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)

        # 7. Prepare optimizer
        optimizer = torch.optim.Adam([learned_latents], lr=lr)
        # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, total_steps=num_opti_steps)

        # 8. Denoising loop with latent optimization
        self._num_timesteps = len(timesteps)
        with self.progress_bar(total=num_opti_steps+1) as progress_bar:
            for optim_step in range(num_opti_steps+1):

                latents = learned_latents
                self.scheduler.set_timesteps(num_inference_steps, device=device)
                for i, t in enumerate(timesteps):

                    # predict the noise residual
                    with torch.no_grad():
                        noise_pred = self.unet(
                            latents,
                            t,
                            encoder_hidden_states=condition_embedding,
                            return_dict=False,
                        )[0]

                    latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample

                image = self.decode_latents(latents)

                if optim_step < num_opti_steps:
                    vertices = apply_displacement(template, image)

                    clothing = collision_loss(vertices, body_vertices, body_normal)
                    clothing = (torch.exp(clothing)-1).sum()

                    std_loss = torch.abs(torch.std(learned_latents)-1)
                    mean_loss = torch.abs(torch.mean(learned_latents))

                    loss = clothing * clothing_weight

                    if target is not None:
                        dist = (vertices - target).pow(2).sum(-1).sqrt()
                        accel_loss = dist.mean()
                        loss += accel_loss * acceleration_weight
                        losses['accel'].append(accel_loss.detach().cpu().item()*0.1)
                    
                    losses['clothing'].append(clothing.detach().cpu().item())
                    losses['std'].append(std_loss.detach().cpu().item()*0.1)
                    losses['mean'].append(mean_loss.detach().cpu().item()*0.1)

                    optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(learned_latents, 1.0)
                    optimizer.step()
                    # scheduler.step()

                    postfix = dict()
                    for k in losses.keys():
                        postfix[k] = losses[k][-1]
                    progress_bar.set_postfix(postfix)
                    
                progress_bar.update()

        if not decoded:
            image = latents
        
        # Offload all models
        self.maybe_free_model_hooks()

        return image.detach().requires_grad_(False)
